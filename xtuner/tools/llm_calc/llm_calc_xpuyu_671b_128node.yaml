# system params
nodes: 128
gpu_per_node: 8
capacity: 1.0

# parallel params
zero_stage: 3
ep: 1
pp: 1
vpp: 1
tp: 1
TP_sp: true
etp: 1
cp: 1
ulysses: 1

# data type
param_type: 2
grad_type: 2
act_type: 2
linear_act_type: 2  # todo: fp8?
os_type: 4
master_type: 4
master_grad_type: 4

# train params
MBS: 1  # 2 for ep domino intra_layer_micro_batch_size
L: 16384
MBN: 1

# model params
model: "DeepSeek-V3"
vocab_size: 129280
hidden_size: 7168
num_hidden_layers: 61

# max_position_embeddings: 163840
attn_type: "mla"  # "mha" or "mla"
kv_lora_rank: 512
q_lora_rank: 1536
num_attention_heads: 128
num_key_value_heads: 128
qk_nope_head_dim: 128
qk_rope_head_dim: 64
v_head_dim: 128
flash_attn: true

# first_k_dense_replace: 3
# moe_layer_freq: 1
# n_group: 8
# topk_group: 4
# topk_method: "noaux_tc"
# norm_topk_prob: true

n_shared_experts: 1
moe_intermediate_size: 2048
num_experts_per_tok: 8
mlp_act_dim: 2
num_experts: 256
topk_weights_position: "after_fc2"

# num_nextn_predict_layers: 1

chunk_loss_size: 1024

# recompute params
recompute: true
