# 实测： 8k 输入，静态显存 21.x G，checkpoint 0.75 G，fp32 logits 4.6 * 2 G (bf16 logits 4.6 G)
# 输出结果与上面一致： static_mem: 21.2603702545166 GiB, total_ckp: 0.75 GiB, head_acts: 4.69921875 GiB (bf16)
# system params
nodes: 1
gpu_per_node: 8
capacity: 1.0

# parallel params
zero_stage: 3
ep: 1
pp: 1
vpp: 1
tp: 1
TP_sp: true
etp: 1
cp: 1
ulysses: 1

# data type
param_type: 2
grad_type: 2
act_type: 2
linear_act_type: 2
os_type: 4
master_type: 4
master_grad_type: 4

# train params
MBS: 1
L: 8192
MBN: 1

# model params
model: "Qwen3-30B-A3B"
vocab_size: 151936
hidden_size: 2048
num_hidden_layers: 24

num_attention_heads: 32
num_key_value_heads: 4
head_dim: 128
flash_attn: true

moe_intermediate_size: 768
num_experts_per_tok: 8
mlp_act_dim: 2
num_experts: 128
topk_weights_position: "after_fc2"

# recompute params
recompute: true
